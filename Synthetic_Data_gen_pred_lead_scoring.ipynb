{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##############Synthetic Data Geneation for Predictive Lead Scoring ##########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Sample Data Generation\n",
    "data = []\n",
    "for i in range(1000):  # Generate 1000 leads\n",
    "    lead_id = i + 1\n",
    "    name = f\"Lead {lead_id}\"\n",
    "    age = random.randint(25, 60)\n",
    "    job_title = random.choice([\"Marketing Manager\", \"Sales Executive\", \"Software Engineer\", \"CEO\", \"Product Manager\", \"Data Analyst\", \"Business Analyst\"])\n",
    "    company_size = random.choice([50, 100, 200, 300, 500, 1000])\n",
    "    industry = random.choice([\"Technology\", \"Finance\", \"Retail\", \"Healthcare\", \"Consulting\"])\n",
    "    location = random.choice([\"New York\", \"Chicago\", \"San Francisco\", \"Boston\", \"Seattle\", \"Austin\", \"Denver\", \"Miami\"])\n",
    "    email_engagement_score = random.randint(0, 100)\n",
    "    website_visits = random.randint(0, 30)\n",
    "    content_downloads = random.randint(0, 10)\n",
    "    last_interaction_date = (datetime.now() - timedelta(days=random.randint(0, 30))).date()\n",
    "    conversion = random.choice([0, 1])  # Binary target\n",
    "\n",
    "    data.append([lead_id, name, age, job_title, company_size, industry, location, email_engagement_score, website_visits, content_downloads, last_interaction_date, conversion])\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data, columns=[\"Lead ID\", \"Name\", \"Age\", \"Job Title\", \"Company Size\", \"Industry\", \"Location\", \"Email Engagement Score\", \"Website Visits\", \"Content Downloads\", \"Last Interaction Date\", \"Conversion\"])\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using open AI \n",
    "\n",
    "Explanation\n",
    "OpenAI API: The code uses the OpenAI API to generate synthetic lead data. It creates a prompt asking the model to generate a lead record in JSON format.\n",
    "\n",
    "Data Structure: The response from the API is expected to be a JSON string that represents the lead data.\n",
    "\n",
    "Evaluating Response: The code uses eval() to convert the string response to a Python dictionary. Note that eval() should be used with caution; ensure the API response is sanitized and trusted.\n",
    "\n",
    "Creating DataFrame: The generated records are stored in a DataFrame for further analysis or processing.\n",
    "\n",
    "CSV Output: The generated data can be saved to a CSV file for later use.\n",
    "\n",
    "Note\n",
    "Replace 'YOUR_API_KEY' with your actual OpenAI API key.\n",
    "Monitor your API usage, as generating a large number of entries may incur costs based on your usage tier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generates 5 million entries in batches, writing them to a CSV file incrementally.\n",
    "\n",
    "Explanation of Modifications\n",
    "Batch Processing: The code generates leads in batches of batch_size (1000 in this case). This reduces the load on the API and avoids hitting rate limits.\n",
    "\n",
    "Appending to CSV: Each batch is written to a CSV file incrementally. The header parameter is set to True only for the first batch, so the column names are written only once.\n",
    "\n",
    "Efficiency: This approach ensures you donâ€™t need to hold all 5 million records in memory at once, making it more memory efficient.\n",
    "\n",
    "Notes\n",
    "API Limitations: Monitor your usage and adjust the batch_size as necessary to stay within your rate limits and budget.\n",
    "Error Handling: Consider adding error handling to manage potential API call failures.\n",
    "File Size: The resulting CSV file will be large. Ensure your environment can handle large files efficiently.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import pandas as pd\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Set your OpenAI API key\n",
    "openai.api_key = 'YOUR_API_KEY'\n",
    "\n",
    "def generate_lead_data(num_entries):\n",
    "    data = []\n",
    "    \n",
    "    for _ in range(num_entries):\n",
    "        prompt = (\n",
    "            \"Generate a lead record with the following attributes: \"\n",
    "            \"Lead ID, Name, Age, Job Title, Company Size, Industry, \"\n",
    "            \"Location, Email Engagement Score, Website Visits, \"\n",
    "            \"Content Downloads, Last Interaction Date, Conversion (0 or 1). \"\n",
    "            \"Provide the output in JSON format.\"\n",
    "        )\n",
    "        \n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "\n",
    "        # Parse the JSON response\n",
    "        lead_record = response['choices'][0]['message']['content']\n",
    "        data.append(eval(lead_record))  # Convert string to dictionary\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Generate synthetic data\n",
    "num_entries = 1000  # Number of leads to generate\n",
    "synthetic_data = generate_lead_data(num_entries)\n",
    "\n",
    "# Create DataFrame from the generated data\n",
    "df = pd.DataFrame(synthetic_data)\n",
    "print(df.head())\n",
    "\n",
    "# Optional: Save to CSV\n",
    "df.to_csv('synthetic_lead_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save the generated data directly to an Amazon S3 bucket,\n",
    " ----move to readme file \n",
    "Install Boto3: If you haven't already, install the boto3 library:\n",
    "\n",
    "bash\n",
    "Copy code\n",
    "pip install boto3\n",
    "AWS Credentials: Ensure you have AWS credentials configured. You can set them in the ~/.aws/credentials file or use environment variables.\n",
    "\n",
    "S3 Bucket: Make sure you have an S3 bucket created where you want to store the CSV file.\n",
    "----\n",
    "Explanation of Modifications\n",
    "Boto3 Library: The boto3 library is used to interact with AWS services, including S3.\n",
    "\n",
    "S3 Configuration: Set the S3_BUCKET_NAME and S3_OBJECT_NAME variables to define where the CSV will be uploaded.\n",
    "\n",
    "Uploading to S3: The upload_to_s3 function uploads the CSV file to the specified S3 bucket after all data has been generated.\n",
    "\n",
    "Error Handling: Added basic error handling for the S3 upload to catch any issues that arise.\n",
    "\n",
    "Notes\n",
    "AWS Permissions: Ensure your AWS credentials have the necessary permissions to upload files to the specified S3 bucket.\n",
    "File Name in S3: The CSV file is uploaded with the same name as specified in S3_OBJECT_NAME. You can customize this if needed.\n",
    "Efficiency: This approach generates the file locally and uploads it once after all data is generated, which may help with performance compared to uploading in smaller increments. If you prefer, you can modify it to upload batches directly after they are created."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import pandas as pd\n",
    "import json\n",
    "import boto3\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Set your OpenAI API key\n",
    "openai.api_key = 'YOUR_API_KEY'\n",
    "\n",
    "# AWS S3 configuration\n",
    "S3_BUCKET_NAME = 'your-s3-bucket-name'  # Replace with your bucket name\n",
    "S3_OBJECT_NAME = 'synthetic_lead_data.csv'  # Name of the file in S3\n",
    "\n",
    "def generate_lead_data(num_entries, batch_size):\n",
    "    for batch_index in range(num_entries // batch_size):\n",
    "        batch_data = []\n",
    "        for _ in range(batch_size):\n",
    "            prompt = (\n",
    "                \"Generate a lead record with the following attributes: \"\n",
    "                \"Lead ID, Name, Age, Job Title, Company Size, Industry, \"\n",
    "                \"Location, Email Engagement Score, Website Visits, \"\n",
    "                \"Content Downloads, Last Interaction Date, Conversion (0 or 1). \"\n",
    "                \"Provide the output in JSON format.\"\n",
    "            )\n",
    "            \n",
    "            response = openai.ChatCompletion.create(\n",
    "                model=\"gpt-3.5-turbo\",\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "            )\n",
    "\n",
    "            # Parse the JSON response\n",
    "            lead_record = response['choices'][0]['message']['content']\n",
    "            batch_data.append(json.loads(lead_record))  # Convert string to dictionary\n",
    "\n",
    "        # Create a DataFrame from the batch data\n",
    "        batch_df = pd.DataFrame(batch_data)\n",
    "\n",
    "        # Append to CSV (you can set header only for the first batch)\n",
    "        if batch_index == 0:\n",
    "            batch_df.to_csv(S3_OBJECT_NAME, mode='w', index=False)\n",
    "        else:\n",
    "            batch_df.to_csv(S3_OBJECT_NAME, mode='a', index=False, header=False)\n",
    "\n",
    "    # Upload to S3 after all batches are generated\n",
    "    upload_to_s3(S3_OBJECT_NAME)\n",
    "\n",
    "def upload_to_s3(file_name):\n",
    "    s3_client = boto3.client('s3')\n",
    "    try:\n",
    "        s3_client.upload_file(file_name, S3_BUCKET_NAME, file_name)\n",
    "        print(f\"File {file_name} uploaded to {S3_BUCKET_NAME} successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error uploading file to S3: {e}\")\n",
    "\n",
    "# Parameters\n",
    "total_entries = 5000000  # Total number of leads to generate\n",
    "batch_size = 1000        # Number of leads to generate in each API call\n",
    "\n",
    "# Generate synthetic data\n",
    "generate_lead_data(total_entries, batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " each batch of data is uploaded to the S3 bucket immediately after it is generated\n",
    " Explanation of Modifications\n",
    "Batch Data Creation: Each batch of generated lead data is saved to a local CSV file named batch_{batch_index + 1}.csv.\n",
    "\n",
    "Immediate Upload: After creating each batch, the code immediately uploads the CSV file to the specified S3 bucket.\n",
    "\n",
    "Local Storage: Each batch is saved locally before being uploaded, allowing for easy management of files and enabling you to handle any potential upload issues.\n",
    "\n",
    "Notes\n",
    "Temporary Storage: If you want to avoid storing files locally after uploading, you can delete them after the upload using os.remove(local_file_name).\n",
    "Monitoring and Error Handling: This code prints messages for successful uploads and errors, which can help you monitor the process.\n",
    "Performance: Depending on your network speed and the AWS region, uploading a large number of files may take some time, so you may want to adjust batch sizes or include progress indicators as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import pandas as pd\n",
    "import json\n",
    "import boto3\n",
    "from datetime import datetime\n",
    "\n",
    "# Set your OpenAI API key\n",
    "openai.api_key = 'YOUR_API_KEY'\n",
    "\n",
    "# AWS S3 configuration\n",
    "S3_BUCKET_NAME = 'your-s3-bucket-name'  # Replace with your bucket name\n",
    "\n",
    "def generate_lead_data(num_entries, batch_size):\n",
    "    for batch_index in range(num_entries // batch_size):\n",
    "        batch_data = []\n",
    "        for _ in range(batch_size):\n",
    "            prompt = (\n",
    "                \"Generate a lead record with the following attributes: \"\n",
    "                \"Lead ID, Name, Age, Job Title, Company Size, Industry, \"\n",
    "                \"Location, Email Engagement Score, Website Visits, \"\n",
    "                \"Content Downloads, Last Interaction Date, Conversion (0 or 1). \"\n",
    "                \"Provide the output in JSON format.\"\n",
    "            )\n",
    "            \n",
    "            response = openai.ChatCompletion.create(\n",
    "                model=\"gpt-3.5-turbo\",\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "            )\n",
    "\n",
    "            # Parse the JSON response\n",
    "            lead_record = response['choices'][0]['message']['content']\n",
    "            batch_data.append(json.loads(lead_record))  # Convert string to dictionary\n",
    "\n",
    "        # Create a DataFrame from the batch data\n",
    "        batch_df = pd.DataFrame(batch_data)\n",
    "\n",
    "        # Save the batch DataFrame to a CSV file locally\n",
    "        local_file_name = f'batch_{batch_index + 1}.csv'\n",
    "        batch_df.to_csv(local_file_name, index=False)\n",
    "\n",
    "        # Upload the batch to S3\n",
    "        upload_to_s3(local_file_name)\n",
    "\n",
    "def upload_to_s3(file_name):\n",
    "    s3_client = boto3.client('s3')\n",
    "    try:\n",
    "        s3_client.upload_file(file_name, S3_BUCKET_NAME, file_name)\n",
    "        print(f\"File {file_name} uploaded to {S3_BUCKET_NAME} successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error uploading file to S3: {e}\")\n",
    "\n",
    "# Parameters\n",
    "total_entries = 5000000  # Total number of leads to generate\n",
    "batch_size = 1000        # Number of leads to generate in each API call\n",
    "\n",
    "# Generate synthetic data\n",
    "generate_lead_data(total_entries, batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data generated in batches is uploaded directly to the S3 bucket without storing any files locally,\n",
    "DataFrame is converted to a CSV string and then upload that string to S3\n",
    "\n",
    "Explanation of Modifications\n",
    "StringIO Buffer: The io.StringIO class is used to create an in-memory buffer that behaves like a file. This allows you to store the CSV data in memory instead of writing it to disk.\n",
    "\n",
    "Direct Upload to S3: After converting the DataFrame to a CSV string, the code uses s3_client.put_object to upload the string directly to S3.\n",
    "\n",
    "No Local Storage: This version of the code does not create any local files, ensuring that all data handling happens in memory.\n",
    "\n",
    "Notes\n",
    "Memory Considerations: Generating large batches may increase memory usage. Ensure that your environment has enough memory to handle the data being processed.\n",
    "Performance: Depending on your network speed and S3 upload limits, the process may take time. You may want to monitor or log the process for better tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import pandas as pd\n",
    "import json\n",
    "import boto3\n",
    "import io\n",
    "\n",
    "# Set your OpenAI API key\n",
    "openai.api_key = 'YOUR_API_KEY'\n",
    "\n",
    "# AWS S3 configuration\n",
    "S3_BUCKET_NAME = 'your-s3-bucket-name'  # Replace with your bucket name\n",
    "\n",
    "def generate_lead_data(num_entries, batch_size):\n",
    "    for batch_index in range(num_entries // batch_size):\n",
    "        batch_data = []\n",
    "        for _ in range(batch_size):\n",
    "            prompt = (\n",
    "                \"Generate a lead record with the following attributes: \"\n",
    "                \"Lead ID, Name, Age, Job Title, Company Size, Industry, \"\n",
    "                \"Location, Email Engagement Score, Website Visits, \"\n",
    "                \"Content Downloads, Last Interaction Date, Conversion (0 or 1). \"\n",
    "                \"Provide the output in JSON format.\"\n",
    "            )\n",
    "            \n",
    "            response = openai.ChatCompletion.create(\n",
    "                model=\"gpt-3.5-turbo\",\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "            )\n",
    "\n",
    "            # Parse the JSON response\n",
    "            lead_record = response['choices'][0]['message']['content']\n",
    "            batch_data.append(json.loads(lead_record))  # Convert string to dictionary\n",
    "\n",
    "        # Create a DataFrame from the batch data\n",
    "        batch_df = pd.DataFrame(batch_data)\n",
    "\n",
    "        # Convert DataFrame to CSV string\n",
    "        csv_buffer = io.StringIO()\n",
    "        batch_df.to_csv(csv_buffer, index=False)\n",
    "        csv_buffer.seek(0)  # Move to the beginning of the StringIO object\n",
    "\n",
    "        # Upload the CSV string to S3\n",
    "        upload_to_s3(csv_buffer, f'batch_{batch_index + 1}.csv')\n",
    "\n",
    "def upload_to_s3(csv_buffer, file_name):\n",
    "    s3_client = boto3.client('s3')\n",
    "    try:\n",
    "        s3_client.put_object(Bucket=S3_BUCKET_NAME, Key=file_name, Body=csv_buffer.getvalue())\n",
    "        print(f\"File {file_name} uploaded to {S3_BUCKET_NAME} successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error uploading file to S3: {e}\")\n",
    "\n",
    "# Parameters\n",
    "total_entries = 5000000  # Total number of leads to generate\n",
    "batch_size = 1000        # Number of leads to generate in each API call\n",
    "\n",
    "# Generate synthetic data\n",
    "generate_lead_data(total_entries, batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensuring that memory is freed up after each batch is uploaded to S3 by explicitly delete the variables holding the batch data after the upload is complete\n",
    "\n",
    "Key Changes\n",
    "Memory Management: After the upload is complete, the code explicitly deletes the variables holding the batch data (batch_data, batch_df, and csv_buffer) using del. This helps free up memory immediately after each batch is processed and uploaded.\n",
    "\n",
    "No Change in Logic: The overall logic of generating leads, creating DataFrames, and uploading to S3 remains the same.\n",
    "\n",
    "Notes\n",
    "Garbage Collection: Python has an automatic garbage collection mechanism, but explicitly deleting large objects can help free memory faster, especially when dealing with large datasets.\n",
    "Performance: This approach helps manage memory more effectively, particularly useful when generating a significant number of entries in a constrained memory environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import pandas as pd\n",
    "import json\n",
    "import boto3\n",
    "import io\n",
    "\n",
    "# Set your OpenAI API key\n",
    "openai.api_key = 'YOUR_API_KEY'\n",
    "\n",
    "# AWS S3 configuration\n",
    "S3_BUCKET_NAME = 'your-s3-bucket-name'  # Replace with your bucket name\n",
    "\n",
    "def generate_lead_data(num_entries, batch_size):\n",
    "    for batch_index in range(num_entries // batch_size):\n",
    "        batch_data = []\n",
    "        for _ in range(batch_size):\n",
    "            prompt = (\n",
    "                \"Generate a lead record with the following attributes: \"\n",
    "                \"Lead ID, Name, Age, Job Title, Company Size, Industry, \"\n",
    "                \"Location, Email Engagement Score, Website Visits, \"\n",
    "                \"Content Downloads, Last Interaction Date, Conversion (0 or 1). \"\n",
    "                \"Provide the output in JSON format.\"\n",
    "            )\n",
    "            \n",
    "            response = openai.ChatCompletion.create(\n",
    "                model=\"gpt-3.5-turbo\",\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "            )\n",
    "\n",
    "            # Parse the JSON response\n",
    "            lead_record = response['choices'][0]['message']['content']\n",
    "            batch_data.append(json.loads(lead_record))  # Convert string to dictionary\n",
    "\n",
    "        # Create a DataFrame from the batch data\n",
    "        batch_df = pd.DataFrame(batch_data)\n",
    "\n",
    "        # Convert DataFrame to CSV string\n",
    "        csv_buffer = io.StringIO()\n",
    "        batch_df.to_csv(csv_buffer, index=False)\n",
    "        csv_buffer.seek(0)  # Move to the beginning of the StringIO object\n",
    "\n",
    "        # Upload the CSV string to S3\n",
    "        upload_to_s3(csv_buffer, f'batch_{batch_index + 1}.csv')\n",
    "\n",
    "        # Free up memory\n",
    "        del batch_data\n",
    "        del batch_df\n",
    "        del csv_buffer\n",
    "\n",
    "def upload_to_s3(csv_buffer, file_name):\n",
    "    s3_client = boto3.client('s3')\n",
    "    try:\n",
    "        s3_client.put_object(Bucket=S3_BUCKET_NAME, Key=file_name, Body=csv_buffer.getvalue())\n",
    "        print(f\"File {file_name} uploaded to {S3_BUCKET_NAME} successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error uploading file to S3: {e}\")\n",
    "\n",
    "# Parameters\n",
    "total_entries = 5000000  # Total number of leads to generate\n",
    "batch_size = 1000        # Number of leads to generate in each API call\n",
    "\n",
    "# Generate synthetic data\n",
    "generate_lead_data(total_entries, batch_size)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
